{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a468011-5168-4e17-ba3b-12caf7e5f619",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Refine model ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df0a3192-ff5d-4418-ba9c-f690cfbd3adf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/brucesherin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.util import bigrams, ngrams, everygrams\n",
    "import scipy\n",
    "from nltk import FreqDist, sent_tokenize\n",
    "from nltk.tokenize.punkt import PunktSentenceTokenizer, PunktParameters\n",
    "\n",
    "# model building\n",
    "from nltk.lm import MLE, Vocabulary\n",
    "from nltk.util import everygrams\n",
    "from nltk.lm.preprocessing import padded_everygram_pipeline, pad_both_ends, flatten\n",
    "\n",
    "# cross entropy\n",
    "import math\n",
    "\n",
    "# file importing\n",
    "import re\n",
    "from nltk.corpus import reader\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "import string\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9a809d6-fa8f-482c-b354-a7725db6422f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing local .txt files to nltk\n",
    "# f1 is r/A2C\n",
    "f1 = open(r'C:\\Users\\HP\\prj\\bsherin\\learning-trajectories\\reddit-outputs\\all-a2c-output.txt', encoding = \"utf8\")\n",
    "raw1 = f1.read()\n",
    "# f2 is r/magicTCG\n",
    "f2 = open(r'C:\\Users\\HP\\prj\\bsherin\\learning-trajectories\\reddit-outputs\\all-magicTCG-output.txt', encoding = \"utf8\")\n",
    "raw2 = f2.read()\n",
    "# f3 is r/A2C from 2016-2017\n",
    "# f4 is r/A2C from 2021-2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edb165c7-42fa-45ae-b021-17ff8fd27835",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# importing local .txt files to nltk\n",
    "# f1 is r/A2C from 2021-2022\n",
    "f1 = open(r'/Users/brucesherin/PycharmProjects/learning-trajectories/subreddits/a2c-2021-01-2022-01-output.txt', encoding = \"utf8\")\n",
    "# f1 = open(r'C:\\Users\\HP\\prj\\bsherin\\learning-trajectories\\reddit-outputs\\a2c-2021-01-2022-01-output.txt', encoding = \"utf8\")\n",
    "raw1 = f1.read()\n",
    "# f2 is r/A2C from 2016-2017\n",
    "f2 = open(r'/Users/brucesherin/PycharmProjects/learning-trajectories/subreddits/a2c-2016-01-2017-01-output.txt', encoding = \"utf8\")\n",
    "# f2 = open(r'C:\\Users\\HP\\prj\\bsherin\\learning-trajectories\\reddit-outputs\\a2c-2016-01-2017-01-output.txt', encoding = \"utf8\")\n",
    "raw2 = f2.read()\n",
    "#f3 is "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99872540-8c48-4198-aed9-7d3dbe88bc43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# tokenize into sentences\n",
    "sents1 = sent_tokenize(raw1)\n",
    "sents2 = sent_tokenize(raw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "34337104-7b8b-418d-91f0-7c45b505a9db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filters out digits, punctuation, links, and stopwords\n",
    "def clean(sent):\n",
    "    \n",
    "    cleaned_sent_with_stopwords = re.sub(r'[^a-zA-Z\\s]|\\bhttps?\\S+\\b', '', sent)\n",
    "    \n",
    "    # removing stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    pattern = r'\\b(?:' + '|'.join(map(re.escape, stop_words)) + r')\\b' # group the non special characters into a single string\n",
    "    text_clean = re.sub(pattern, '', cleaned_sent_with_stopwords, flags = re.IGNORECASE)    \n",
    "    return text_clean.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "247af3b4-7621-43d0-8dcf-2062526a2ea6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Bruce's version\n",
    "# filters out digits, punctuation, links, and stopwords\n",
    "# removing stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "pattern = r'\\b(?:' + '|'.join(map(re.escape, stop_words)) + r')\\b' # group the non special characters into a single string\n",
    "def clean(sent):\n",
    "    cleaned_sent_with_stopwords = re.sub(r'[^a-zA-Z\\s]|\\bhttps?\\S+\\b', '', sent)\n",
    "    text_clean = re.sub(pattern, '', cleaned_sent_with_stopwords, flags = re.IGNORECASE)    \n",
    "    return text_clean.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e70a0a2d-424a-44f1-991a-02b82a51b565",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# handles our new line cases\n",
    "def split_add(sent_lst):\n",
    "    split_sent_lst = []\n",
    "    for i in range(len(sent_lst)):\n",
    "        if '\\n' in sent_lst[i]:\n",
    "            temp_split = re.split(r'\\n', sent_lst[i])\n",
    "            split_sent_lst.append(temp_split[0])\n",
    "            split_sent_lst.append(temp_split[1])\n",
    "        else:\n",
    "            split_sent_lst.append(sent_lst[i])\n",
    "    return split_sent_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747ea0f4-de86-463f-a318-617baeb285e5",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "The code below takes 70 seconds to run with Louis' version clean defined abov. With mine it takea bout 15 seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "473915ee-b649-4474-b1a3-5cd7aa40fd5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 70.29091191291809 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# process text\n",
    "import time\n",
    "start_time = time.time()\n",
    "sents1_cleaned = [clean(sentence) for sentence in split_add(sents1)]\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "# sents1_test = [clean(sentence) for sentence in split_add(sents1[0:50])] # CHANGE TO MAKE FULL DATASET"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3f6e82-acbf-4383-a97a-7fd91ab86212",
   "metadata": {},
   "source": [
    "## Note\n",
    "How well does this tokenizer do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9e49e36a-12a6-4fa0-a457-e10f34cd91e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def sent_split(sentence):\n",
    "    sent_split = re.split(r'\\s+', sentence.strip())\n",
    "    return sent_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "40b4f179-c7c2-4e69-ba62-15184794acc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# split the elements of each sentence into words\n",
    "sents1_split = [sent_split(sentence) for sentence in sents1_cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a63c588e-ba81-4fea-b218-956a8f342701",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create bigrams from our list of words\n",
    "def bi_grams(sent_lst):\n",
    "    output_lst = []\n",
    "    for sentence in sent_lst:\n",
    "        output_lst.extend(list(bigrams(sentence)))\n",
    "    return output_lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe31e2c-80a1-4ebb-b357-08a815d2e690",
   "metadata": {},
   "source": [
    "## Note\n",
    "The below function recomputes the freqdist every time.\n",
    "\n",
    "Also it builds the freqdist from sents1_cleaned which isn't tokenized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b71443ca-041a-4ef5-a734-311ebf31a0db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fdist = nltk.FreqDist(sents1_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957fdfcd-1464-4e1e-b0aa-8f2ad9af670e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fdist.most_common(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ab9ae74-5682-4d95-aec0-95c34f201a8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CLEAN UP - make the sents1_cleaned not hard-coded\n",
    "def replace_unknowns(sentence, dataset, cutoff): \n",
    "    unk_token = '<UNK>'\n",
    "    word_freq = nltk.FreqDist(dataset) # MANUALLY ADJUST\n",
    "    return [word if word_freq[word] > cutoff \n",
    "            else unk_token for word in sentence]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce654f4-0976-446f-bf57-d237bad035da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sents1_split_with_unk = [replace_unknowns(sent, sents1_cleaned, 1) for sent in sents1_split]\n",
    "# temp val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbee41d-8286-421e-8421-ce800ae52763",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# make our model\n",
    "train, vocab = padded_everygram_pipeline(2, sents1_split_with_unk) # TEMP AMT\n",
    "lm = MLE(2)\n",
    "lm.fit(train, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "1e5d0259-b4e6-4870-95ff-a19845301c32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def counter(word):\n",
    "    count = 0\n",
    "    for sentence in sents1_cleaned[0:100]:\n",
    "        if word in sentence:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7351b455-8d1b-4e31-ba60-f90ea1aeb292",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate conditional probabilities (excluding the bigrams that only occur once)\n",
    "prob_w2_given_w1 = {}\n",
    "# TEMP AMT BELOW\n",
    "for sentence in sents1_split_with_unk: # so that we don't do frequencies for words below cutoff but the total count isn't affected by our unk cutoff\n",
    "    for word1 in sentence:\n",
    "        for word2, freq in lm.counts[[word1]].items():\n",
    "            word1_freq = lm.counts[word1]\n",
    "            tuple_temp = (word1, word2)\n",
    "            if(freq != word1_freq):\n",
    "                prob_w2_given_w1[tuple_temp] = freq/word1_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34e67d3-55a3-49db-9932-c8b01e5f7f99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# temporary filter\n",
    "rm_list = []\n",
    "for (key1, key2) in prob_w2_given_w1:\n",
    "    if prob_w2_given_w1[(key1, key2)] == 1.0:\n",
    "        rm_list.append((key1, key2))\n",
    "for key_tup in rm_list:\n",
    "    prob_w2_given_w1.pop(key_tup, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de11ad7-485d-4648-9dba-56cc2af9f7fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now process our test data (r/magicTCG)\n",
    "sents2_cleaned = [clean(sentence) for sentence in split_add(sents2)] # temp val.\n",
    "sents2_split = [sent_split(sentence) for sentence in sents2_cleaned]\n",
    "sents2_split_with_unk = [replace_unknowns(sent, sents2_cleaned, 1) for sent in sents2_split]\n",
    "sents2_bigrams = padded_everygram_pipeline(2, sents2_split_with_unk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "0f4f5cd5-3e54-49ce-b3ed-cf6f49ba1e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents1_bigrams = padded_everygram_pipeline(2, sents1_split_with_unk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "aa20f12d-20bc-4f58-9ba0-1e89d95ebecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_dict = {('the', 'cat'): 0.7, ('cat', 'sat'): 0.8, ('sat', 'on'): 0.6, ('on', 'the'): 0.4, ('the', 'mat'): 0.5}\n",
    "test_seq = [('the', 'cat'), ('cat', 'sat'), ('sat', 'on'), ('on', 'the'), ('the', 'mat')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "40551202-8df8-4af5-a15f-ec320568941e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use cross-entropy\n",
    "# where word_seq is a list of bigrams from text we want to test\n",
    "# and lm_data is the outputted probability distribution of different bigrams from our model\n",
    "def cross_entropy(word_seq, lm_data):\n",
    "    N = 4 # MANUALLY ADJUST - hard coded\n",
    "    ce = 0.0\n",
    "    # cycle thru each bigram and if there's a match w/ any of the bigram index keys in our probability list,\n",
    "    # then we just use that probability, but if not we use a small, non-zero value\n",
    "    for i in range(0, N - 1):\n",
    "        bigram_temp = word_seq[i]\n",
    "        probability = 1e-20\n",
    "        if(bigram_temp in lm_data.keys()):\n",
    "            probability = lm_data[bigram_temp] # might have to cast to tuple?\n",
    "        ce += probability\n",
    "    avg_prob = -1 * math.log(ce)/N\n",
    "    return avg_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "3b47af1a-6272-4b05-8086-bced3b48c45c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.18548433618234433"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy(test_seq, test_dict) # should output -0.56"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56053210-7587-4dee-b251-e005c7547731",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sents2_bigrams' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cross_entropy(\u001b[43msents2_bigrams\u001b[49m, prob_w2_given_w1)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sents2_bigrams' is not defined"
     ]
    }
   ],
   "source": [
    "cross_entropy(sents2_bigrams, prob_w2_given_w1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
